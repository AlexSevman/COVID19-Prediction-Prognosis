{
 "cells": [
  {
<<<<<<< HEAD
=======
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for creating a beautiful table"
   ]
  },
  {
>>>>>>> funtions_graphs
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generation of a beauti-table\n",
    "def beautitable(data):\n",
    "    table = PrettyTable(['Variable',\n",
    "                         'Data_type',\n",
    "                         'Non-NaN',\n",
    "                         'NaN',\n",
    "                         'Unique',\n",
    "                         'Example',\n",
    "                        'Median',\n",
    "                        'Max',\n",
    "                        'Min'])\n",
    "    for i in data.columns:\n",
    "        table.add_row([i, \n",
    "                       data[i].dtype,\n",
    "                       len(data[i]) - np.sum(data[i].isnull()),\n",
    "                       np.sum(data[i].isnull()),\n",
    "                       np.count_nonzero(data[i].unique()),\n",
    "                       data[~data[i].isnull()][i].iloc[0],\n",
    "                       data[i].median(),\n",
    "                       data[i].max(),\n",
    "                       data[i].min()\n",
    "                    ])\n",
    "    print(table)\n",
    "    print()\n",
    "    return           "
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for creating plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_sns(data, feature, target ,perc = None, n = None, title_graph = None, title_legend = None):\n",
    "    total = len(data[feature])\n",
    "    count = data[feature].nunique()\n",
    "\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count +10, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n +10, 5))\n",
    "    plt.xticks(#rotation = 45, \n",
    "               fontsize = 12)\n",
    "    plt.suptitle(title_graph, fontsize = 25, y=1.1)\n",
    "    ax = sns.countplot(\n",
    "        data = data, \n",
    "        x = feature, \n",
    "        hue = target, \n",
    "        palette = \"rocket\",\n",
    "        edgecolor=sns.color_palette(\"dark\"),\n",
    "        linewidth=2,\n",
    "        order=data[feature].value_counts().index[:n]\n",
    "    )\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )\n",
    "        elif perc==False:\n",
    "            label = p.get_height()\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        x = p.get_x() + p.get_width()\n",
    "        y = p.get_height()\n",
    "        \n",
    "        ax.annotate(\n",
    "            label, \n",
    "            (x, y),\n",
    "            ha = 'center',\n",
    "            va = 'top',\n",
    "            size = 10, \n",
    "            xytext = (-15, 20),\n",
    "            textcoords = 'offset points'\n",
    "        )\n",
    "        sns.move_legend(ax, \"lower center\", bbox_to_anchor=(.85, .8), ncol=3, title=title_legend, frameon=False)\n",
    "        ax.set_xlabel('Prognosis', fontsize=16)\n",
    "        ax.set_ylabel('Number of subjects', fontsize=16)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(data, feature, perc = False, n = None, title_graph = None):\n",
    "    total = len(data[feature])\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count +2, 6))\n",
    "    else:\n",
    "        plt.figure(figsize=(n +2, 6))\n",
    "    plt.xticks(rotation = 90, fontsize = 15)\n",
    "    plt.title(title_graph)\n",
    "    ax = sns.countplot(\n",
    "        data = data, \n",
    "        x = feature, \n",
    "        palette = 'Paired',\n",
    "        #order=data[feature].value_counts().index[:n]\n",
    "    )\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )\n",
    "        else:\n",
    "            label = p.get_height()\n",
    "            \n",
    "        x = p.get_x() + p.get_width()\n",
    "        y = p.get_height()\n",
    "        \n",
    "        ax.annotate(\n",
    "            label, \n",
    "            (x, y),\n",
    "            ha = 'center',\n",
    "            va = 'top',\n",
    "            size = 12, \n",
    "            xytext = (-30, 10),\n",
    "            textcoords = 'offset points'\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating histograms and boxplot for univariate analysis of numerical variables\n",
    "def his_boxplot(data, feature, figsize=(20, 3), kde=False, bins = None, title = None, x_label = None,):\n",
    "    g, (ax_box, ax_hist) = plt.subplots(\n",
    "        ncols=2, \n",
    "        sharex=True,\n",
    "        #gridspec_kw={'height_ratios':(0.25, 0,75)},\n",
    "        figsize = figsize,\n",
    "    )\n",
    "    plt.suptitle(title, fontsize = 25,\n",
    "              fontweight = 20,\n",
    "              color = 'black',\n",
    "              y=1.2\n",
    "              )\n",
    "    plt.xlabel(x_label)\n",
    "    sns.histplot(\n",
    "        data = data, x = feature,kde=True, color='red', ax=ax_hist, bins= bins\n",
    "    ) #if bins else sns.histplot(data = data, x = feature,kde=kde, ax=ax_hist)\n",
    "\n",
    "    sns.boxplot(data = data, x = feature, ax=ax_box, showmeans= True, color='blue')\n",
    "    # For histagram: add a vertical line with mean\n",
    "    ax_hist.axvline(\n",
    "        data[feature].mean(), color='violet', linestyle = '--')\n",
    "    ax_box.axvline(\n",
    "        data[feature].median(), color='orange', linestyle = 'dashdot')\n",
    "    #ax_box.set_title(title, fontsize = 25)\n",
    "    #ax_hist.set_title(title, fontsize = 25)\n",
    "    ax_hist.set(ylabel = 'Number of subjects')\n",
    "    ax_hist.set(xlabel = x_label)\n",
    "    ax_box.set(xlabel = x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred, \n",
    "                          pos_label = '1')  # to compute Recall\n",
    "    precision = precision_score(target, pred, pos_label='1')  # to compute Precision\n",
    "    f1 = f1_score(target, pred, pos_label='1')  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target, title=None,ax=None):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    Y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, Y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\", ax=ax)\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model,flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    #Predicting on train and tests\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    #Accuracy of the model\n",
    "    train_acc = model.score(X_train,Y_train)\n",
    "    test_acc = model.score(X_test,Y_test)\n",
    "    \n",
    "    #Recall of the model\n",
    "    train_recall = metrics.recall_score(Y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(Y_test,pred_test)\n",
    "    \n",
    "    #Precision of the model\n",
    "    train_precision = metrics.precision_score(Y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(Y_test,pred_test)\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",model.score(X_train,Y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,Y_test))\n",
    "        print(\"Recall on training set : \",metrics.recall_score(Y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(Y_test,pred_test))\n",
    "        print(\"Precision on training set : \",metrics.precision_score(Y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(Y_test,pred_test))\n",
    "    \n",
    "    return score_list # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impor_feat (model, title=None):\n",
    "    feature_names = X.columns\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(f'Feature Importances, {title}')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_plot(model):\n",
    "    feature_names = X_train.columns\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    out = tree.plot_tree(\n",
    "        model,\n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        fontsize=9,\n",
    "        node_ids=True,\n",
    "        class_names=True,\n",
    "    )\n",
    "    for o in out:\n",
    "        arrow = o.arrow_patch\n",
    "        if arrow is not None:\n",
    "            arrow.set_edgecolor(\"black\")\n",
    "            arrow.set_linewidth(1)\n",
    "    plt.show()"
   ]
>>>>>>> funtions_graphs
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
